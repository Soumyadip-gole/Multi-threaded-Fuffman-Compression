# ðŸ—œï¸ Parallel Huffman Compressor (Rust)

A **multi-threaded file compressor and decompressor** written in Rust, implementing **Huffman encoding/decoding** and a **custom thread pool** to process multiple files in parallel.

This project focuses on **systems fundamentals**: concurrency, task scheduling, and CPU-bound parallelism â€” not on inventing a new compression algorithm.

---

## âœ¨ Features

* âœ… **Huffman Encoding & Decoding**

  * Byte-frequency analysis
  * Huffman tree construction
  * Bit-level encoding and decoding
  * Lossless round-trip verification

* ðŸ§µ **Custom Thread Pool**

  * Fixed-size worker pool
  * Threads are spawned once and reused
  * Workers wait for tasks instead of exiting
  * Controlled concurrency (no thread explosion)

* âš¡ **Parallel File Processing**

  * One file = one task
  * Multiple files compressed / decompressed concurrently
  * Scales with available CPU cores

* ðŸ“Š **Performance-Oriented**

  * CPU-bound workload
  * Benchmarkable against single-threaded execution

---

## ðŸ§  Design Overview

### Why Huffman Coding?

Huffman coding is a classic entropy encoding algorithm that:

* Is simple enough to implement correctly
* Clearly demonstrates compression fundamentals
* Is widely used as a building block in real compressors

This project intentionally avoids more complex algorithms (LZMA, arithmetic coding) to keep the focus on **concurrency and correctness**.

---

### Why a Thread Pool?

Creating threads repeatedly (`thread::spawn`) is expensive and unscalable.

Instead, this project uses:

* A **fixed number of worker threads**
* A **shared task queue**
* Workers that continuously:

  1. wait for a task
  2. execute it
  3. return to waiting

This mirrors how real-world CPU-bound systems handle parallel work.

---

### Parallelism Model

```
Main Thread
 â”œâ”€â”€ submit file1
 â”œâ”€â”€ submit file2
 â”œâ”€â”€ submit file3

Thread Pool (N workers)
 â”œâ”€â”€ Worker 1 â†’ Huffman(file1)
 â”œâ”€â”€ Worker 2 â†’ Huffman(file2)
 â”œâ”€â”€ Worker 3 â†’ Huffman(file3)
 â””â”€â”€ Worker 4 â†’ waiting
```

* No shared mutable state between tasks
* No locking during compression work
* Safe and predictable parallelism

---

## ðŸ—‚ï¸ Project Structure

```
src/
 â”œâ”€â”€ huffman/
 â”‚   â”œâ”€â”€ frequency.rs    # Frequency table
 â”‚   â”œâ”€â”€ tree.rs         # Huffman tree
 â”‚   â”œâ”€â”€ encode.rs       # Encoding logic
 â”‚   â””â”€â”€ decode.rs       # Decoding logic
 â”‚
 â”œâ”€â”€ thread_pool/
 â”‚   â”œâ”€â”€ mod.rs
 â”‚   â””â”€â”€ pool.rs         # Custom thread pool
 â”‚
 â”œâ”€â”€ cli.rs              # Argument parsing
 â””â”€â”€ main.rs             # Entry point
```

---

## ðŸš€ Usage

### Compress files

```bash
cargo run -- compress input_dir/ output_dir/
```

### Decompress files

```bash
cargo run -- decompress input_dir/ output_dir/
```

Each file is processed independently and may be handled by a different worker thread.

---

## ðŸ“ˆ Performance Notes

* Parallel speedup depends on:

  * number of CPU cores
  * number of input files
  * file sizes

* Huffman tree construction is sequential **per file**, but **files are processed in parallel**.

This mirrors how many real-world compressors parallelize work at the **job level**, not inside the core algorithm.

---

## âš ï¸ Limitations

* Huffman-only compression (no dictionary-based compression)
* Not designed to beat tools like 7-Zip or WinRAR
* Optimized for learning and correctness, not maximum compression ratio

---

## ðŸŽ¯ Learning Outcomes

This project demonstrates:

* Safe concurrency in Rust
* Thread pool design
* CPU-bound parallel task execution
* Bit-level data processing
* Clear separation of concerns

---

## ðŸ§© Future Improvements (Optional)

* Block-level compression for large files
* Progress reporting using atomics
* Benchmark harness
* Panic recovery in worker threads

---

## ðŸ“Œ Final Note

> This project is not about inventing new compression algorithms â€”
> it is about **understanding and implementing real systems concepts correctly**.
